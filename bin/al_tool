#!/usr/bin/env python
import sys
import os
import logging
from contextlib import contextmanager
from functools import partial
from decimal import Decimal, getcontext
getcontext().prec = 80

# Hack in order to avoid making this a python package
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(sys.argv[0])), os.pardir))

import argparse
from pymongo import Connection

from algolab.simplify import anglereduce, rdp
from algolab.combine import anglecombine
from algolab.db import empty, dedup, delonelynize, new, copy, recalculate_distances
from algolab.cmdutil import log_progress, log_change, require_col, defaultparser
from algolab.segment import StationESSegmenter
from algolab.simplify import simplify as simplify2
from algolab.stations import (cluster_stations, build_station_collection,
                              build_station_collection_from_stations,
                              generate_railviz_station_file)


STATION_COLLECTION = 'stations'
RAILWAY_GRAPH_COLLECTION = 'railway_graph'


def prepare_stations(args):
    db = Connection(args.host, args.port)[args.db]
    require_col(db, 17)
    with log_progress("Building Station Collection"):
        if args.sync:
            build_station_collection(db['railway_graph_17'], db[STATION_COLLECTION],
                                     args.stations_file, args.routes_file)
        else:
            build_station_collection_from_stations(db['railway_graph_17'],
                                                   db[STATION_COLLECTION],
                                                   args.stations_file)

    with log_progress("Generating Station Filter File (%s)" % args.station_filter_file):
        generate_railviz_station_file(args.station_usage_file,
                                      args.station_filter_file)


def prepare_railway_graph(args):
    db = Connection(args.host, args.port)[args.db]
    rg = db["railway_graph_17"]
    with log_progress("Cleaning"):
        require_col(db, RAILWAY_GRAPH_COLLECTION)
        empty(rg)

        copy(db[RAILWAY_GRAPH_COLLECTION], rg)
        c0 = rg.count()

        # This is the general cleaning step
        with log_progress("Removing lonely nodes"):
            delonelynize(rg)
            c1 = rg.count()
            log_change(c1, c0)

        with log_progress("Removing duplicates"):
            dedup(rg)
            c2 = rg.count()
            log_change(c2, c1)

        with log_progress("Recalculating all distances"):
            recalculate_distances(rg)


def filter_rg(args):
    if args.all:
        zls = range(8, 17)
    else:
        assert len(args.zl) > 0, \
            "Specify at least one zoomlevel or use '--all-zoomlevels parameter"
        zls = args.zl
    db = Connection(args.host, args.port)[args.db]

    require = lambda *x: require_col(db, x)

    require(STATION_COLLECTION)
    stations = db[STATION_COLLECTION]
    station_ids = set(s["_id"] for s in stations.find())

    # the segmenter
    segmenter = partial(StationESSegmenter, station_ids)
    simplify = partial(simplify2, segmenter=segmenter)

    # Collections for zoom level 8 - 16
    rgs = {i: db["railway_graph_%i" % i] for i in range(8, 18)}

    ## Zoom Level 16-12 (RDP)
    for zli, zl in enumerate([16, 15, 14, 13, 12], start=1):
        if zl in zls:
            with log_progress("Zoom Level %s" % zl):
                require(zl + 1)
                empty(rgs[zl])

                # max tolerance in m
                epsilon = zli * 1.6

                with log_progress("Applying RDP with eps=%f" % epsilon):
                    simplify(rdp, rgs[zl + 1], rgs[zl], [epsilon])
                    log_change(rgs[zl].count(), rgs[zl + 1].count())

    ## Zoom Level 11 (Clustering + RDP)
    if 11 in zls:
        with log_progress("Zoom Level 11"):
            require(12)
            empty(rgs[11])

            cluster_stations(rgs[12], stations, db['clustered'])
            log_change(db['clustered'].count(), rgs[12].count())

            # max tolerance in m
            epsilon = 6

            with log_progress("Applying RDP with eps=%f" % epsilon):
                simplify(rdp, db['clustered'], rgs[11], [epsilon])
                zls.remove(11)
                log_change(rgs[11].count(), db['clustered'].count())

    ## Zoom Level 10 (Anglecombine + RDP)
    if 10 in zls:
        with log_progress("Zoom Level 10"):
            require(11)
            empty(db["parallel"])
            empty(rgs[10])

            # max tolerance in degrees
            epsilon = 10

            with log_progress("Applying Anglecombine with eps=%f" % epsilon):
                copy(rgs[11], db["parallel"])

                anglecombine(db["parallel"], epsilon, keep_ids=station_ids)
                log_change(db["parallel"].count(), rgs[11].count())

            epsilon = 6
            with log_progress("Applying RDP with eps=%f" % epsilon):
                simplify(rdp, db["parallel"], rgs[10], [epsilon])
                log_change(rgs[10].count(), db["parallel"].count())

    ## Zoom Level 9-8 (RDP)
    for zli, zl in enumerate([9, 8], start=1):
        if zl in zls:
            with log_progress("Zoom Level %s" % zl):
                require(zl + 1)
                empty(rgs[zl])

                # max tolerance in m
                epsilon = zli * 10

                with log_progress("Applying RDP with eps=%f" % epsilon):
                    simplify(rdp, rgs[zl + 1], rgs[zl], [epsilon])
                    log_change(rgs[zl].count(), rgs[zl + 1].count())

if __name__ == "__main__":
    dparser = defaultparser()
    dparser.add_argument("--no-progress", action="store_true", dest="np",
                         default=False, help="don't print the progress")
    parser = argparse.ArgumentParser(description="Algorithm Lab Tool",
                                     parents=[dparser])
    subs = parser.add_subparsers()

    rg_parser = subs.add_parser('prepare', help='prepare railway graph for filtering', parents=[dparser])
    rg_parser.set_defaults(function=prepare_railway_graph)

    station_parser = subs.add_parser('stations', help='Prepare Stations', parents=[dparser])
    station_parser.add_argument("-s", "--stations-file", type=str, default="data/Stations.txt",
                        help="path to station file [defaults to data/Stations.txt]")
    station_parser.add_argument("-u", "--station-usage-file", default="data/stationUsage.txt",
                        help="path to station file [defaults to data/Stations.txt]")
    station_parser.add_argument("-r", "--routes-file", type=str, default="data/sgrv.csv",
                        help="path to routes file [defaults to data/sgrv.csv]")
    station_parser.add_argument("-f", "--station-filter-file", type=str, default="ZoomLevelStations.txt",
                                help="where to write the railviz filter file"
                                " [defaults to ZoomLevelStations.txt]")
    station_parser.add_argument("--sync-with-routes", action="store_true", dest="sync",
                                help="only include stations that are mentioned in routes file "
                                "[defaults to false]")
    station_parser.set_defaults(function=prepare_stations)

    filter_parser = subs.add_parser('filter', help='Filter the railway graph', parents=[dparser])
    filter_parser.add_argument("zl", type=int, nargs="*",
                               help="zoom levels to generate a RG for")
    filter_parser.add_argument("--all-zoomlevels", action="store_true", dest="all",
                               help="generate for all zoomlevels (8-16)")
    filter_parser.set_defaults(function=filter_rg)

    args = parser.parse_args()

    with log_progress("Overall process"):
        args.function(args)
